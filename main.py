#!/usr/bin/env python3
"""
avito-scraper ‚Äî –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –ê–≤–∏—Ç–æ —Å –Ω–∞–¥–µ–∂–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ü–µ–Ω—ã –∏ –ª–æ–∫–∞—Ü–∏–∏
"""

import requests
from bs4 import BeautifulSoup
import time
import csv
import os
import argparse
import re
from urllib.parse import quote, urljoin

# =============== –ö–û–ù–°–¢–ê–ù–¢–´ ===============
BASE_URL = "https://www.avito.ru"

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ä—É—Å—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π —Ä–µ–≥–∏–æ–Ω–æ–≤ –≤ URL
REGIONS = {
    "–º–æ—Å–∫–≤–∞": "moskva",
    "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥": "sankt-peterburg", 
    "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": "ekaterinburg",
    "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": "novosibirsk",
    "–∫–∞–∑–∞–Ω—å": "kazan",
    "–Ω—É—Ä-—Å—É–ª—Ç–∞–Ω": "nur-sultan",
    "–º–∏–Ω—Å–∫": "minsk",
    "–∫–∏–µ–≤": "kiev",
}

# =============== –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ===============
def get_avito_page_content(url):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç HTTP-–∑–∞–ø—Ä–æ—Å –∫ —Å–∞–π—Ç—É Avito –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Referer': 'https://www.avito.ru/',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'same-origin',
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        time.sleep(2)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        return BeautifulSoup(response.text, 'html.parser')
    except requests.exceptions.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}")
        return None

def safe_extract_text(element, default="–ù–µ —É–∫–∞–∑–∞–Ω–æ"):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞."""
    if element and element.text.strip():
        return element.text.strip()
    return default

def parse_avito_ads(soup):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏—è—Ö —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏.
    """
    ads = []
    
    # –ù–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
    selectors = [
        'div[data-marker="item"]',
        'div.iva-item-root',
        'div.items-item',
        'div.js-item',
        'article'
    ]
    
    items = []
    for selector in selectors:
        found_items = soup.select(selector)
        if found_items:
            items = found_items
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º '{selector}': {len(items)}")
            break
    
    if not items:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π.")
        # –°–æ—Ö—Ä–∞–Ω–∏–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        with open("debug_page.html", "w", encoding="utf-8") as f:
            f.write(soup.prettify())
        print("‚ö†Ô∏è HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ debug_page.html –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return []

    for item in items:
        try:
            # ===== –ó–ê–ì–û–õ–û–í–û–ö –ò –°–°–´–õ–ö–ê =====
            title = "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            link = "–ë–µ–∑ —Å—Å—ã–ª–∫–∏"
            
            title_selectors = [
                'a[data-marker="item-title"]',
                'h3[itemprop="name"]',
                'a.iva-item-title',
                'h3.title',
                'a.link-link'
            ]
            
            for selector in title_selectors:
                title_elem = item.select_one(selector)
                if title_elem:
                    title = title_elem.get('title', '') or title_elem.text.strip()
                    href = title_elem.get('href', '')
                    if href:
                        link = urljoin(BASE_URL, href)
                    break
            
            # ===== –¶–ï–ù–ê =====
            price = "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
            price_selectors = [
                'meta[itemprop="price"]',
                'span[data-marker="item-price"]',
                'div.iva-item-price',
                'span.price',
                'p.price',
                'div[data-marker="item-price"]'
            ]
            
            for selector in price_selectors:
                price_elem = item.select_one(selector)
                if price_elem:
                    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å content –∞—Ç—Ä–∏–±—É—Ç –¥–ª—è meta —Ç–µ–≥–∞
                    if price_elem.name == 'meta' and price_elem.get('content'):
                        price = price_elem['content']
                    else:
                        price = price_elem.text.strip()
                    break
            
            # –û—á–∏—â–∞–µ–º —Ü–µ–Ω—É –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            if price != "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞":
                price = re.sub(r'[^\d\s]', '', price).strip()
                price = re.sub(r'\s+', ' ', price)  # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
            
            # ===== –õ–û–ö–ê–¶–ò–Ø =====
            location = "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
            location_selectors = [
                'div[data-marker="item-address"]',
                'span[data-marker="item-address"]',
                'div.geo-address',
                'span.address',
                'div.item-address',
                'span[class*="address"]'
            ]
            
            for selector in location_selectors:
                loc_elem = item.select_one(selector)
                if loc_elem:
                    location = loc_elem.text.strip()
                    break
            
            # ===== –î–ê–¢–ê ===== (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ)
            date = "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
            date_selectors = [
                'div[data-marker="item-date"]',
                'span.date',
                'div.item-date'
            ]
            
            for selector in date_selectors:
                date_elem = item.select_one(selector)
                if date_elem:
                    date = date_elem.text.strip()
                    break
            
            # –≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è CSV
            title = title.replace(";", ",").replace("\n", " ").replace("\r", " ")
            location = location.replace(";", ",").replace("\n", " ").replace("\r", " ")
            
            ad_data = {
                "title": title,
                "price": price,
                "location": location,
                "date": date,
                "link": link
            }
            
            ads.append(ad_data)
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
            continue
            
    return ads

def save_to_csv(data, filename="avito_ads.csv"):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ CSV-—Ñ–∞–π–ª.
    """
    if not data:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")
        return

    os.makedirs("data", exist_ok=True)
    filepath = f"data/{filename}"
    
    try:
        with open(filepath, 'w', newline='', encoding='utf-8-sig') as csvfile:
            fieldnames = ['title', 'price', 'location', 'date', 'link']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            for ad in data:
                writer.writerow(ad)
                
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª {filepath}")
        print(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ CSV: {e}")

def main():
    parser = argparse.ArgumentParser(description='–ü–∞—Ä—Å–µ—Ä –ê–≤–∏—Ç–æ ‚Äî —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è')
    parser.add_argument('--query', required=True, help='–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å, –Ω–∞–ø—Ä–∏–º–µ—Ä: "–Ω–æ—É—Ç–±—É–∫–∏"')
    parser.add_argument('--region', default='moskva', help='–†–µ–≥–∏–æ–Ω (—Ä—É—Å—Å–∫–æ–µ –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ)')
    parser.add_argument('--min-price', type=int, help='–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞')
    parser.add_argument('--max-price', type=int, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞')
    parser.add_argument('--max-pages', type=int, default=3, help='–ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3)')
    parser.add_argument('--debug', action='store_true', help='–°–æ—Ö—Ä–∞–Ω–∏—Ç—å HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏')

    args = parser.parse_args()

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞
    region_lower = args.region.strip().lower()
    final_region = REGIONS.get(region_lower, region_lower)

    print(f"üîç –ü–æ–∏—Å–∫: {args.query}")
    print(f"üìç –†–µ–≥–∏–æ–Ω: {final_region}")
    print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü: {args.max_pages}")
    if args.min_price:
        print(f"üí∞ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞: {args.min_price}")
    if args.max_price:
        print(f"üí∞ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞: {args.max_price}")
    print("-" * 50)

    all_ads = []
    base_search_url = f"{BASE_URL}/{final_region}?q={quote(args.query)}"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ü–µ–Ω—ã
    if args.min_price:
        base_search_url += f"&pmin={args.min_price}"
    if args.max_price:
        base_search_url += f"&pmax={args.max_price}"

    for page in range(1, args.max_pages + 1):
        url = f"{base_search_url}&p={page}"
        print(f"üåê –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {url}")
        
        soup = get_avito_page_content(url)
        if not soup:
            if page == 1:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É.")
                return
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}")
            continue

        if args.debug and page == 1:
            with open("debug_page.html", "w", encoding="utf-8") as f:
                f.write(soup.prettify())
            print("‚ö†Ô∏è HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ debug_page.html")

        ads = parse_avito_ads(soup)
        if not ads:
            print(f"‚ö†Ô∏è –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            if page == 1:
                print("‚ÑπÔ∏è –°–æ–≤–µ—Ç: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å —Ä–µ–≥–∏–æ–Ω–∞ –∏ –∑–∞–ø—Ä–æ—Å–∞")
            break

        all_ads.extend(ads)
        print(f"‚úÖ –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {len(ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        next_button = soup.find('a', {'data-marker': 'pagination-next'})
        if not next_button:
            print("üîö –≠—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
            break

        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
        time.sleep(1)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if all_ads:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        safe_query = re.sub(r'[^\w\s-]', '', args.query)[:30]
        filename = f"avito_{safe_query}_{final_region}_{timestamp}.csv"
        save_to_csv(all_ads, filename)
        
        # –í—ã–≤–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        print("\nüìã –ü—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π:")
        for i, ad in enumerate(all_ads[:3], 1):
            print(f"{i}. {ad['title'][:50]}... - {ad['price']} - {ad['location']}")
    else:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")

if __name__ == "__main__":
    main()